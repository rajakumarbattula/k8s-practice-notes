Kubernetes - selfie shot
========================

-> According to the kubernetes.io website, Kubernetes is:
	"an open-source system for automating deployment, scaling, and management of containerized applications"
-> A key aspect of Kubernetes is that it builds on 15 years of experience at Google in a project called borg.
-> Kubernetes is written in Go Language
-> Communication to, as well as internally, between components is API call-driven, which allows for flexibility. Configuration 
	information is stored in a JSON format, but is most often written in YAML. Kubernetes agents convert the YAML to JSON 
	prior to persistence to the database. 
-> Built on open source and easily extensible, Kubernetes is definitely a solution to manage containerized applications.
-> Alternatives are Docker swarm, Apache Mesos, Nomad from HashiCorp, Rancher
-> Kubernetes Architecture :
	Master Node ( Control-pane ) :
		API Server : you can communicate with the API using a local client called 'kubectl'or you can write your own client.
		kube-scheduler : The kube-scheduler sees the requests for running containers coming to the API and finds a suitable node to run that container in.
		Kube Controller : This component is used to maintain the state of the cluster.
		Cloud Controller : This manages cloud services such as load balancing, networks, storage
		etcd ( storage ) : Acts like data base. Used to store state of the cluster, history etc in terms of key value pairs. It works with curl and other HTTP libraries, and 
			provides reliable watch queries.
	Worker node :
		Kubelet : Agent running on each node placed inside the cluster. The kubelet receives requests to run the containers, manages any necessary resources and watches over them on the local node.
		Kube-proxy : The kube-proxy creates and manages networking rules to expose the container on the network using iptables entries.
		Container Runtime : To manage containers in side the cluster. We must install container run time such as docker, containerd, rkt etc
-> Kubernetes has the following main components:
	- Master and worker nodes
	- Controllers
	- Services
	- Pods of containers
	- Namespaces and quotas
	- Network and policies
	- Storage.
-> Supervisord is a lightweight process monitor used in traditional Linux environments to monitor and notify about other 
	processes. In the cluster, this daemon monitors both the kubelet and docker processes. It will try to restart them if they fail, 
	and log events.
-> Kubernetes does not have cluster-wide logging yet. Instead, another CNCF project is used, called Fluentd. When 
	implemented, it provides a unified logging layer for the cluster, which filters, buffers, and routes messages.
-> Pods : The whole point of Kubernetes is to orchestrate the lifecycle of a container. Pod is the wrapper of one or many sidecar containers
-> While Kubernetes orchestration does not allow direct manipulation on a container level, we can manage the resources containers are allowed to consume. 
-> In the resources section of the PodSpec you can pass parameters which will be passed to the container runtime on the 
			scheduled node: 
				resources:
				  limits:
				    cpu: "1"
				    memory: "4Gi"
				  requests:
				    cpu: "0.5"
					memory: "500Mi
-> Another way to manage resource usage of the containers is by creating a ResourceQuota object, which allows hard and 
	soft limits to be set in a namespace. The quotas allow management of more resources than just CPU and memory and 
	allows limiting several objects.
-> Services :
		-> ClusterIP : pods are internally communicates using ClusterIP services
		-> NodePort : To expose our process to the Internet, we use NodePort. It is used to test our application. NodePort ranges from 30000 to 32768
		-> LoadBalancer : To expose our application to the Internet, we use LoadBalancer. It is acts as single entry to our application.
		-> ExternalName : Services of type ExternalName map a Service to a DNS name, not to a typical selector such as my-service or cassandra. You specify these Services with the spec.externalName parameter.
		-> Headless service :
-> Ingress : Used to allow HTTP/HTTPS traffic to your application.
-> Controller :
-> Single IP per POD : A pod represents a group of co-located containers with some associated data volumes. All containers in a pod share the 
	same network namespace.
-> POD to POD communication : 
	While a CNI plugin can be used to configure the network of a pod and provide a single IP per pod, CNI does not help you 
	with pod-to-pod communication across nodes.
		• The requirement from Kubernetes is the following:
		• All pods can communicate with each other across nodes.
		• All nodes can communicate with all pods.
		• No Network Address Translation (NAT).
		• Basically, all IPs involved (nodes and pods) are routable without NAT. This can be achieved at the physical network 
		infrastructure if you have access to it (e.g. GKE). Or, this can be achieved with a software defined overlay with solutions like:
			- Weave
			- Flannel
			- Calico
			- Romana​.
-> Installing a Pod Network
		• Prior to initializing the Kubernetes cluster, the network must be considered and IP conflicts avoided. There are several Pod networking choices, 
		in varying levels of development and feature set: 
			• Calico
			A flat Layer 3 network which communicates without IP encapsulation, used in production with software such as Kubernetes, OpenShift, 
			Docker, Mesos and OpenStack. Viewed as a simple and flexible networking model, it scales well for large environments. Another network 
			option, Canal, also part of this project, allows for integration with Flannel. Allows for implementation of network policies. 
			• Flannel
			A Layer 3 IPv4 network between the nodes of a cluster. Developed by CoreOS, it has a long history with Kubernetes. Focused on traffic 
			between hosts, not how containers configure local networking, it can use one of several backend mechanisms, such as VXLAN. A flanneld
			agent on each node allocates subnet leases for the host. While it can be configured after deployment, it is much easier prior to any Pods being 
			added. 
			• Kube-router
			Feature-filled single binary which claims to "do it all". The project is in the alpha stage, but promises to offer a distributed load balancer, firewall, 
			and router purposely built for Kubernetes. 
			• Romana
			Another project aimed at network and security automation for cloud native applications. Aimed at large clusters, IPAM-aware topology and 
			integration with kops clusters. 
			• Weave Net
			Typically used as an add-on for a CNI-enabled Kubernetes cluster. 
		• Many of the projects will mention the Container Network Interface (CNI), which is a CNCF project. Several container runtimes currently use 
		CNI. As a standard to handle deployment management and cleanup of network resources, it will become more popular
-> Pod :
	a simple pod manifest in YAML format. You can see the apiVersion (it must match the existing API group), the kind (the type of object to create), 
	the metadata (at least a name), and its spec (what to create and parameters), which define the container that actually runs in this pod: 
	
	apiVersion: v1
	kind: Pod
	metadata:
	  name: firstpod
	spec:
	  containers:
	    - image: nginx
	      name: stan
	
	we can use following command to create simple pod using above manifest named simplepod.yaml
	
	kubectl create -f simplepod.yaml
	kubectl run NAME --image = image [--env = "key = value"] [--port = port] [--replicas = replicas] [--dry-run = bool] [--overrides = inline-json] [--command] --[COMMAND] [args...]
	kubectl get pods
	kubectl get pod firstpod -o yaml
	kubectl get pod firstpod -o json
	
	kubectl run dev-web --image=nginx:1.13.7-alpine
	kubectl run dev-web --image=nginx:1.13.7-alpine --dry-run -o yaml
	
	apiVersion: v1
	kind: Pod
	metadata:
	  labels:
		run: dev-web
	  name: dev-web
	spec:
	  containers:
	  - image: nginx:1.13.7-alpine
		name: dev-web 
	
	The primary tool used from the command line will be kubectl, which calls curl on your behalf. You can also use the 
	curl command from outside the cluster to view or make changes. 
	
	kubectl config view
	
	- If you view the verbose output from a previous page, you will note that the first line references a configuration file 
	where this information is pulled from, ~/.kube/config
	- The output on the previous page shows multiple lines of output, with each of the keys being heavily truncated. While the keys may 
	look similar, close examination shows them to be distinct:
	apiVersion : As with other objects, this instructs the kube-apiserver where to assign the data.
	clusters : This contains the name of the cluster, as well as where to send the API calls. The certificate-authority-data is passed to authenticate the curl request. 
	- contexts : A setting which allows easy access to multiple clusters, possibly as various users, from one configuration file. It can be used to set 
	namespace, user, and cluster. 
	- current-context : Shows which cluster and user the kubectl command would use. These settings can also be passed on a per-command basis. 
	- kind : Every object within Kubernetes must have this setting, in thus case a declaration of object type Config. 
	- preferences : Currently not used, optional settings for the kubectl command, such as colorizing output. 
	- users : A nickname associated with client credentials, which can be client key and certificate, username and password, and a token. Token 
	and username/password are mutually exclusive. These can be configured via the kubectl config set-credentials command. 
	
- Namespaces : 
	- default : Every API call include a namespace uses dafault namespace.
	- kube-node-lease : This namespace holds Lease objects associated with each node.
	- kube-system : Contains infrastructure pods.
	- kube-public : A namespace readable by all, even those not authenticated. General information is often included in this namespaces.
	
	kubectl get ns
	kubectl create ns namespace-name 
	
	or 
	
	apiVersion: v1
	kind: Namespace
	metadata:
	  name: <insert-namespace-name-here>
	  
	kubectl create pod samplepod.yaml -n mynamespace
	
	kubectl get pods -A or kubectl get pod --all-namespaces
	kubectl get pods -n kube-system
	kubectl describe ns <namespace-name>
	kubectl get ns mynamespace -o yaml or kubectl get ns/mynamespace -o yaml
	
	Ex:
	cat redis.yaml
	
	apiVersion: v1
	kind: Pod
	metadata: 
		name: redis
		namespace: test-ns
	
	
	- Namespaces are logically isolate the application between different teams
	- But pods created between different namespaces will communicate each other because network is not isolated between different namespaces in the cluster.
	
Deployments :
	- The default controller for a container deployed via kubectl run is a Deployment. 
	-  As with other objects, a deployment can be made from a YAML or JSON spec file. When added to the cluster, the controller 
	will create a ReplicaSet and a Pod automatically. The containers, their settings and applications can be modified via an 
	update, which generates a new ReplicaSet, which, in turn, generates new Pods.
	- The updated objects can be staged to replace previous objects as a block or as a rolling update, which is determined as part 
	of the deployment specification. Most updates can be configured by editing a YAML file and running kubectl apply. You can 
	also use kubectl edit to modify the in-use configuration. Previous versions of the ReplicaSets are kept, allowing a rollback to 
	return to a previous configuration.
	- ReplicationControllers (RC) ensure that a specified number of pod replicas is running at any one time. 
	ReplicationControllers also give you the ability to perform rolling updates. However, those updates are managed on the client 
	side. This is problematic if the client loses connectivity, and can leave the cluster in an unplanned state. To avoid problems 
	when scaling the RCs on the client side, a new resource has been introduced in the extensions/v1beta1 API group: Deployments. 
	- Deployments allow server-side updates to pods at a specified rate. They are used for canary and other deployment patterns. 
	Deployments generate ReplicaSets, which offer more selection features than ReplicationControllers, such as 
	matchExpressions.
	[ Note : RC's are legacy and RS's are latest. Both will do the same, But for RC we use equality based selectors while for RS we use set based selectors ]
	
	- apiVersion :
		- A value of v1 shows that this object is considered to be a stable resource. In this case, it is not the deployment. It is a 
		reference to the List type.
		- The dash is a YAML indication of the first item, which declares the apiVersion of the object as extensions/v1beta1. While 
		this would indicate that the object is not considered stable and will be dynamic, deployments are the suggested object to 
		use. 
	- kind : The kind of the resource is deployment.
	- metadata of deployment : 
		- This is where we would find name, labels, annotations, namespace, creationTimestamp, genration, resourceVersion, selfLink, uid.
	- spec of the deployment : 
		- There are two spec declarations for the deployment. The first will modify the ReplicaSet created, while the second will pass along the Pod configuration.
		Ex : 
			spec:
			  replicas: 1
			  selector:
			    matchLabels:
			      run: dev-web
			  strategy:
			    rollingUpdate: 
				  maxSurge: 1 
				  maxUnavailable: 1
			    type: RollingUpdate
			  template:
				metadata:
				  labels:
					app: nginx
				spec:
				  containers:
				  - name: nginx
					image: nginx:1.14.2
					ports:
					- containerPort: 80
				
	• spec : A declaration that the following items will configure the object being created. 
	• replicas : As the object being created is a ReplicaSet, this parameter determines how many Pods should be created. If you 
		were to use kubectl edit and change this value to two, a second Pod would be generated. 
	• selector : A collection of values ANDed together. All must be satisfied for the replica to match. Do not create Pods which match 
	these selectors, as the deployment controller may try to control the resource, leading to issues. 
	• matchLabels : Set-based requirements of the Pod selector. Often found with the matchExpressions statement, to further 
		designate where the resource should be scheduled. 
	• strategy : A header for values having to do with updating Pods. Works with the later listed type. Could also be set to Recreate, 
		which would delete all existing pods before new pods are created. With RollingUpdate, you can control how many Pods are 
		deleted at a time with the following parameters. 
	• maxsurge : Maximum number of Pods over desired number of Pods to create. Can be a percentage, default of 25%, or an 
		absolute number. This creates a certain number of new Pods before deleting old ones, for continued access. 
	• maxUnavailable: A number or percentage of Pods which can be in a state other than Ready during the update process.
	• type : Even though listed last in the section, due to the level of white space indentation, it is read as the type of object being
		configured. (e.g. RollingUpdate).
	• template : Data being passed to the ReplicaSet to determine how to deploy an object (in this case, containers). 
	• containers : Key word indicating that the following items of this indentation are for a container. 
	• image : This is the image name passed to the container engine, typically Docker. The engine will pull the image and create the Pod. 
	• imagePullPolicy : Policy settings passed along to the container engine, about when and if an image should be downloaded or used from a local cache. 
	• name : The leading stub of the Pod names. A unique string will be appended. 
	• resources : By default, empty. This is where you would set resource restrictions and settings, such as a limit on CPU or memory for the containers. 
	• terminationMessagePath : A customizable location of where to output success or failure information of a container.
	• terminationMessagePolicy : The default value is File, which holds the termination method. It could also be set to FallbackToLogsOnError which will use the last chunk of container log if the message file is empty and the container shows an error.
	• dnsPolicy : Determines if DNS queries should go to kube-dns or, if set to Default, use the node's DNS resolution configuration. 
	• restartPolicy : Used should the container be restarted if killed. Automatic restarts are part of the typical strength of Kubernetes. 
	• scheduleName : Allows for the use of a custom scheduler, instead of the Kubernetes default. 
	• securityContext : Flexible setting to pass one or more security settings, such as SELinux context, AppArmor values, users and UIDs for the containers to use. 
	• terminationGracePeriodSeconds : The amount of time to wait for a SIGTERM to run until a SIGKILL is used to terminate the container.
	
	Ex: sample-deply.yaml
	apiVersion: apps/v1
	kind: Deployment
	metadata:
	  name: nginx-deployment
	  labels:
		app: nginx
	spec:
	  replicas: 3
	  selector:
		matchLabels:
		  app: nginx
	  template:
		metadata:
		  labels:
			app: nginx
		spec:
		  containers:
		  - name: nginx
			image: nginx:1.14.2
			ports:
			- containerPort: 80
	kubectl run mydeploy --image=nginx --port=80 --replicas=3 --dry-run -o yaml 
	kubectl create -f sample-deploy.yaml
	kubectl get deploy
	kubectl get rs
	kubectl get po -o wide
	kubectl edit deploy sample-deploy.yaml
	kubectl delete deploy nginx-deployment
	
- Services : 	
	• As touched on previously, the Kubernetes architecture is built on the concept of transient, decoupled objects connected 
	together. Services are the agents which connect Pods together, or provide access outside of the cluster, with the idea that 
	any particular Pod could be terminated and rebuilt. Typically using Labels, the refreshed Pod is connected and the 
	microservice continues to provide the expected resource via an Endpoint object. Google has been working on Extensible 
	Service Proxy (ESP), based off the nginx HTTP reverse proxy server, to provide a more flexible and powerful object than 
	Endpoints, but ESP has not been adopted much outside of the Google App Engine or GKE environments. 
	• There are several different service types, with the flexibility to add more, as necessary. Each service can be exposed 
	internally or externally to the cluster. A service can also connect internal resources to an external resource, such as a thirdparty database. 
	• The kube-proxy agent watches the Kubernetes API for new services and endpoints being created on each node. It opens 
	random ports and listens for traffic to the ClusterIP:Port, and redirects the traffic to the randomly generated service 
	endpoints. 
	• Services provide automatic load-balancing, matching a label query. While there is no configuration of this option, there is the 
	possibility of session affinity via IP. Also, a headless service, one without a fixed IP nor load-balancing, can be configured. 
	• Unique IP addresses are assigned, and configured via the etcd database, so that Services implement iptables to route 
	traffic, but could leverage other technologies to provide access to resources in the future.
	• Labels are used to determine which Pods should receive traffic from a service. As we have learned, labels can be 
	dynamically updated for an object, which may affect which Pods continue to connect to a service. 
	• The default update pattern is for a rolling deployment, where new Pods are added, with different versions of an application, 
	and due to automatic load balancing, receive traffic along with previous versions of the application. 
	• Should there be a difference in applications deployed, such that clients would have issues communicating with different 
	versions, you may consider a more specific label for the deployment, which includes a version number. When the 
	deployment creates a new replication controller for the update, the label would not match. Once the new Pods have been 
	created, and perhaps allowed to fully initialize, we would edit the labels for which the Service connects. Traffic would shift to 
	the new and ready version, minimizing client version confusion.
- Service Types :
	- ClusterIP : The ClusterIP service type is the default, and only provides access internally (except if manually creating an external 
	endpoint). The range of ClusterIP used is defined via an API server startup option.
	- NodePort : The NodePort type is great for debugging, or when a static IP address is necessary, such as opening a particular 
	address through a firewall. The NodePort range is defined in the cluster configuration. [ 30000 to 32768 ]
	- LoadBalancer : The LoadBalancer service was created to pass requests to a cloud provider like GKE or AWS. Private cloud solutions 
	also may implement this service type if there is a cloud provider plugin, such as with CloudStack and OpenStack. Even 
	without a cloud provider, the address is made available to public traffic, and packets are spread among the Pods in the 
	deployment automatically. 
	- ExternalName : A newer service is ExternalName, which is a bit different. It has no selectors, nor does it define ports or endpoints. It 
	allows the return of an alias to an external service. The redirection happens at the DNS level, not via a proxy or forward. 
	This object can be useful for services not yet brought into the Kubernetes cluster. A simple change of the type in the 
	future would redirect traffic to the internal objects.
	
- kube-proxy :
	• The kubectl proxy command creates a local service to access a ClusterIP. This can be useful for troubleshooting or development work. 
	• The kube-proxy running on cluster nodes watches the API server service resources. It presents a type of virtual IP address for services 
	other than ExternalName. The mode for this process has changed over versions of Kubernetes. 
	• In v1.0, services ran in userspace mode as TCP/UDP over IP or Layer 4. In the v1.1 release, the iptables proxy was added and became the default mode starting with v1.2. 
	• In the iptables proxy mode, kube-proxy continues to monitor the API server for changes in Service and Endpoint objects, 
	and updates rules for each object when created or removed. One limitation to the new mode is an inability to connect to a 
	Pod should the original request fail, so it uses a Readiness Probe to ensure all containers are functional prior to 
	connection. This mode allows for up to approximately 5000 nodes. Assuming multiple Services and Pods per node, this 
	leads to a bottleneck in the kernel. 
	• Another mode beginning in v1.9 is ipvs. While in beta, and expected to change, it works in the kernel space for greater 
	speed, and allows for a configurable load-balancing algorithm, such as round-robin, shortest expected delay, least 
	connection and several others. This can be helpful for large clusters, much past the previous 5000 node limitation. This 
	mode assumes IPVS kernel modules are installed and running prior to kube-proxy. 
	• The kube-proxy mode is configured via a flag sent during initialization, such as mode=iptables and could also be IPVS or 
	userspace.
- Accessing an Application with a Service :
	- The basic step to access a new service is to use kubectl. 
		kubectl expose deployment/nginx --port=80 --type=NodePort
		kubectl get svc
		kubectl expose deploy nginx-deployment --port=80 --type=NodePort --dry-run -o yaml
		
		apiVersion: v1
		kind: Service
		metadata:  
		  labels:
			app: nginx
		  name: nginx-deployment
		spec:
		  ports:
		  - port: 80
			protocol: TCP
			targetPort: 80
		  selector:
			app: nginx
		  type: NodePort
		  
		  kubectl get svc
		  kubectl describe svc nginx-deployment
	- Accessing an Application with a Service : 
		• The kubectl expose command created a service for the nginx deployment. This service used port 80 and generated a 
		random port on all the nodes. A particular port and targetPort can also be passed during object creation to avoid random 
		values. The targetPort defaults to the port, but could be set to any value, including a string referring to a port on a backend 
		Pod. Each Pod could have a different port, but traffic is still passed via the name. Switching traffic to a different port would
		maintain a client connection, while changing versions of software, for example. 
		• The kubectl get svc command gave you a list of all the existing services, and we saw the nginx service, which was created 
		with an internal cluster IP. 
		• The range of cluster IPs and the range of ports used for the random NodePort are configurable in the API server startup
		options. 
		• Services can also be used to point to a service in a different namespace, or even a resource outside the cluster, such as a 
		legacy application not yet in Kubernetes. 
- Volumes :
	- A Kubernetes volume shares the Pod lifetime, not the containers within. Should a container terminate, the data would continue to be available to the new container.
	- A volume is a directory, possibly pre-populated, made available to containers in a Pod. The creation of the directory, the 
	backend storage of the data and the contents depend on the volume type. As of v1.8, there are 25 different volume types 
	ranging from rbd to gain access to Ceph, to NFS, to dynamic volumes from a cloud provider like Google's 
	gcePersistentDisk. Each has particular configuration options and dependencies.
	- An alpha feature to v1.9 is the Container Storage Interface (CSI) with the goal of an industry standard interface for container 
	orchestration to allow access to arbitrary storage systems. Currently, volume plugins are "in-tree", meaning they are 
	compiled and built with the core Kubernetes binaries. This "out-of-tree" object will allow storage vendors to develop a single 
	driver and allow the plugin to be containerized. This will replace the existing Flex plugin which requires elevated access to 
	the host node, a large security concern.
	- Should you want your storage lifetime to be distinct from a Pod, you can use Persistent Volumes. These allow for empty or 
	pre-populated volumes to be claimed by a Pod using a Persistent Volume Claim, then outlive the Pod. Data inside the 
	volume could then be used by another Pod, or as a means of retrieving data. 
	- A Pod specification can declare one or more volumes and where they are made available. Each requires a name, a type, 
	and a mount point. The same volume can be made available to multiple containers within a Pod, which can be a method of 
	container-to-container communication. A volume can be made available to multiple Pods, with each given an access mode
	to write. There is no concurrency checking, which means data corruption is probable, unless outside locking takes place. 
	- particular access mode is part of a Pod request. As a request, the user may be granted more, but not less access, though a 
	direct match is attempted first. The cluster groups volumes with the same mode together, then sorts volumes by size, from 
	smallest to largest. The claim is checked against each in that access mode group, until a volume of sufficient size matches. 
	The three access modes are RWO (ReadWriteOnce), which allows read-write by a single node, ROX (ReadOnlyMany), 
	which allows read-only by multiple nodes, and RWX (ReadWriteMany), which allows read-write by many nodes.
	- When a volume is requested, the local kubelet uses the kubelet_pods.go script to map the raw devices, determine and 
	make the mount point for the container, then create the symbolic link on the host node filesystem to associate the storage to
	the container. The API server makes a request for the storage to the StorageClass plugin, but the specifics of the requests 
	to the backend storage depend on the plugin in use. 
	- If a request for a particular StorageClass was not made, then the only parameters used will be access mode and size. The 
	volume could come from any of the storage types available, and there is no configuration to determine which of the available 
	ones will be used.
- Volume Types :
	• There are several types that you can use to define volumes, each with their pros and cons. Some are local, and many make 
	use of network-based resources.
	• In GCE or AWS, you can use volumes of type GCEpersistentDisk or awsElasticBlockStore, which allows you to mount 
	GCE and EBS disks in your Pods, assuming you have already set up accounts and privileges.
	• emptyDir and hostPath volumes are easy to use. As mentioned, emptyDir is an empty directory that gets erased when the 
	Pod dies, but is recreated when the container restarts. The hostPath volume mounts a resource from the host node 
	filesystem. The resource could be a directory, file socket, character, or block device. These resources must already exist on
	the host to be used. There are two types, DirectoryOrCreate and FileOrCreate, which create the resources on the host, 
	and use them if they don't already exist. 
	• NFS (Network File System) and iSCSI (Internet Small Computer System Interface) are straightforward choices for multiple 
	readers scenarios.
	• rbd for block storage or CephFS and GlusterFS, if available in your Kubernetes cluster, can be a good choice for multiple 
	writer needs.
	• Besides the volume types we just mentioned, there are many other possible, with more being added: azureDisk, azureFile, 
	csi, downwardAPI, fc (fibre channel), flocker, gitRepo, local, projected, portworxVolume, quobyte, scaleIO, secret, 
	storageos, vsphereVolume, persistentVolumeClaim, etc.
- Volume Spec :
	• One of the many types of storage available is an emptyDir. The kubelet will create the directory in the container, but not mount any 
	storage. Any data created is written to the shared container space. As a result, it would not be persistent storage. When the Pod is 
	destroyed, the directory would be deleted along with the container. 
	apiVersion: v1
	kind: Pod
	metadata:
		name: busybox
		namespace: default
	spec:
		containers:
			- image: busybox
			name: busy
			command: 
			- sleep 
			- "3600"
			volumeMounts:
			  - mountPath: /scratch
			    name: scratch-volume
		volumes:
			- name: scratch-volume 
			  emptyDir: {} 
	• The YAML file above would create a Pod with a single container with a volume named scratch-volume created, which would create the /scratch directory inside the container. 
- Shared Volume example : 
	• The following YAML file creates a pod with two containers, both with access to a shared volume:
	containers:
		- image: busybox
		volumeMounts:
		  - mountPath: /busy
		    name: test
		  name: busy
		- image: busybox		  
	    volumeMounts:
	      - mountPath: /box
	        name: test
		  name: box	    
	volumes:
	  - name: test
	    emptyDir: {} 
		
	kubectl exec -it busybox -c box -- touch /box/foobar
	kubectl exec -ti busybox -c busy -- ls -l /busy
	
	• You could use emptyDir or hostPath easily, since those types do not require any additional setup, and will work in your Kubernetes cluster. 
	• Note that one container wrote, and the other container had immediate access to the data. There is nothing to keep the containers
	from overwriting the other's data. Locking or versioning considerations must be part of the application to avoid corruption.

- Persistent Volumes and Claims : 
	• A persistent volume (pv) is a storage abstraction used to retain data longer than the Pod using it. Pods define a volume of type 
	PersistentVolumeClaim (pvc)with various parameters for size and possibly the type of backend storage known as its StorageClass. The 
	cluster then attaches the PersistentVolume. 
	• Kubernetes will dynamically use volumes that are available, irrespective of its storage type, allowing claims to any backend storage. 
	• There are several phases to persistent storage: 
		• Provisioning can be from PVs created in advance by the cluster administrator, or requested from a dynamic source, such as the cloud provider. 
		• Binding occurs when a control loop on the master notices the PVC, containing an amount of storage, access request, and optionally, a 
		particular StorageClass. The watcher locates a matching PV or waits for the StorageClass provisioner to create one. The PV must match at 
		least the storage amount requested, but may provide more. 
		• The use phase begins when the bound volume is mounted for the Pod to use, which continues as long as the Pod requires. 
		• Releasing happens when the Pod is done with the volume and an API request is sent, deleting the PVC. The volume remains in the state from
		when the claim is deleted until available to a new claim. The resident data remains depending on the PersistentVolumeReclaimPolicy. 
		• The reclaim phase has three options:
			- Retain, which keeps the data intact, allowing for an administrator to handle the storage and data.
			- Delete tells the volume plugin to delete the API object, as well as the storage behind it.
			- The Recycle option runs an rm -rf /mountpoint and then makes it available to a new claim. With the stability of dynamic provisioning, the 
			  Recycle option is planned to be deprecated.
			  
	kubectl get pv
	kubectl get pvc
	
	- The following example shows a basic declaration of a PersistentVolume using the hostPath type.
	kind: PersistentVolume
	apiVersion: v1 
	metadata: 
		name: 10Gpv01 
		labels:
			type: local 
	spec: 
		capacity: 
		storage: 10Gi 
		accessModes: 
		  - ReadWriteOnce
		hostPath: 
			path: "/somepath/data01"
	
	• Each type will have its own configuration settings. For example, an already created Ceph or GCE Persistent Disk would not need to be 
	configured, but could be claimed from the provider. 
	• Persistent volumes are not a namespaces object, but persistent volume claims are. An alpha feature of v1.9 allows for static provisioning of 
	Raw Block Volumes, which currently support the Fibre Channel plugin. 	
	• With a persistent volume created in your cluster, you can then write a manifest for a claim and use that claim in your pod 
	definition. In the Pod, the volume uses the PersistentVolumeClaim.
	
	kind: PersistentVolumeClaim
	apiVersion: v1 
	metadata: 
		name: myclaim
	spec: 
		accessModes: 
		  - ReadWriteOnce
		resources:
		requests:
			storage: 8GI 
	
	(In the Pod)
	....
	spec:
	  containers:
	....
	  volumes:
	    - name: test-volume
	      persistentVolumeClaim:
	        claimName: myclaim
	
	• The Pod configuration could also be as complex as this:
	  volumeMounts:
	    - name: Cephpd
	      mountPath: /data/rbd
	volumes:
	  - name: rbdpd
	  rbd:
	    monitors:
			- '10.19.14.22:6789'
			- '10.19.14.23:6789'
			- '10.19.14.24:6789'
	    pool: k8s
		image: client
		fsType: ext4
		readOnly: true
		user: admin
		keyring: /etc/ceph/keyring
		imageformat: "2"
		imagefeatures: "layering"

- Dynamic Provisioning :
	• While handling volumes with a persistent volume definition and abstracting the storage provider using a claim is powerful, 
	a cluster administrator still needs to create those volumes in the first place. Starting with Kubernetes v1.4, 
	Dynamic	Provisioning allowed for the cluster to request storage from an exterior, pre-configured source. API calls made by the 
	appropriate plugin allow for a wide range of dynamic storage use. 
	• The StorageClass API resource allows an administrator to define a persistent volume provisioner of a certain type, passing 
	storage-specific parameters. 
	• With a StorageClass created, a user can request a claim, which the API Server fills via auto-provisioning. The resource will 
	also be reclaimed as configured by the provider. AWS and GCE are common choices for dynamic storage, but other options 
	exist, such as a Ceph cluster or iSCSI. Single, default class is possible via annotation.
	• Here is an example of a StorageClass using GCE: 
		apiVersion: storage.k8s.io/v1 # Recently became stable
		kind: StorageClass
		metadata:
			name: fast # Could be any name
			provisioner: kubernetes.io/gce-pd
			parameters:
				type: pd-ssd
				
- Secrets : 
	• Pods can access local data using volumes, but there is some data you don't want readable to the naked eye. Passwords 
	may be an example. Someone reading through a YAML file may read a password and remember it. Using the Secret API 
	resource, the same password could be encoded. A casual reading would not give away the password. You can create, get, 
	or delete secrets:
	$ kubectl get secrets 
	• Secrets can be manually encoded with kubectl create secret:​
	$ kubectl create secret generic --help 
	$ kubectl create secret generic mysql --from-literal=password=root
	• A secret is not encrypted, only base64-encoded. You can see the encoded string inside the secret with kubectl. The secret 
	will be decoded and be presented as a string saved to a file. The file can be used as an environmental variable or in a new 
	directory, similar to the presentation of a volume. 
	• A secret can be made manually as well, then inserted into a YAML file: 
	$ echo LFTr@1n | base64
	TEZUckAxbgo= 
	$ vim secret.yaml
	apiVersion: v1
	kind: Secret
	metadata:
		name: LF-secret
		data:
			password: TEZUckAxbgo= 
	• An alpha feature in v1.7.0 is the type EncryptionConfig which allows for encryption at rest of secrets. There are four types, 
	checked in configuration order, each attempting to decrypt the data using pre-populated keys. Encryption uses the first type 
	and first key listed in the configuration file. Currently, AES-CBC is considered to be the strongest, but slowest encryption. 
	
- Using Secrets via Environment Variables :
	• A secret can be used as an environmental variable in a Pod. You can see one being configured in the following example:
		 ... 
		spec: 
			containers: 
				- image: mysql:5.5 
				env: 
					- name: MYSQL_ROOT_PASSWORD 
					valueFrom: 
						secretKeyRef: 
							name: mysql
							key: password 
							name: mysql
	• There is no limit to the number of Secrets used, but there is a 1MB limit to their size. Each secret occupies memory, along 
	with other API objects, so very large numbers of secrets could deplete memory on a host.
	• They are stored in the tmpfs storage on the host node, and are only sent to the host running Pod. All volumes requested by 
	a Pod must be mounted before the containers within the Pod are started. So, a secret must exist prior to being requested. 
	
- Mounting Secrets as Volumes : 
	• You can also mount secrets as files using a volume definition in a pod manifest. The mount path will contain a file whose 
	name will be the key of the secret created with the kubectl create secret step earlier.
		... 
		spec: 
		containers: 
		- image: busybox
		command: 
		- sleep 
		- "3600" 
		volumeMounts:
		  - mountPath: /mysqlpassword
			name: mysql
			name: busy 
		volumes: 
			- name: mysql
			secret: 
				secretName: mysql
	• Once the pod is running, you can verify that the secret is indeed accessible in the container:
	$ kubectl exec -it busybox -- cat /mysqlpassword/password 
	LFTr@1n
- Portable Data with ConfigMaps : 
	• A similar API resource to Secrets is the ConfigMap, except the data is not encoded. In keeping with the concept of decoupling in Kubernetes, using a ConfigMap decouples a container image from configuration artifacts.
	• They store data as sets of key-value pairs or plain configuration files in any format. The data can come from a collection of files or all files in a directory. It can also be populated from a literal value. 
	• A ConfigMap can be used in several different ways. A Pod can use the data as environmental variables from one or more sources. The values contained inside can be passed to commands inside the pod.
	 A Volume or a file in a Volume can be created, including different names and particular access modes. In addition, cluster components like controllers can use the data. ​
	• Let's say you have a file on your local filesystem called config.js. You can create a ConfigMap that contains this file. The configmap object will have a data section containing the content of the file:
			$ kubectl get configmap foobar -o yaml
				kind: ConfigMap
				apiVersion: v1 
				metadata:
				name: foobar
				data: 
				config.js: | 
				{ 
				...
	• ConfigMaps can be consumed in various ways:
		• Pod environmental variables from single or multiple ConfigMaps
		• Use ConfigMap values in Pod commands
		• Populate Volume from ConfigMap
		• Add ConfigMap data to specific path in Volume
		• Set file names and access mode in Volume from ConfigMap data
		• Can be used by system components and controllers.
	
- Using ConfigMaps :
	• Like secrets, you can use ConfigMaps as environment variables or using a volume mount. They must exist prior to being 
	used by a Pod, unless marked as optional. They also reside in a specific namespace.
	• In the case of environment variables, your pod manifest will use the valueFrom key and the configMapKeyRef value to 
	read the values. 
	For instance:
		env: 
		  - name: SPECIAL_LEVEL_KEY 
		  valueFrom: 
		    configMapKeyRef: 
		      name: special-config 
		      key: special.how
	• With volumes, you define a volume with the configMap type in your pod and mount it where it needs to be used.
			volumes: 
			  - name: config-volume 
			    configMap: 
			      name: special-config
- DaemonSets : 
	• A newer object to work with is the DaemonSet. This controller ensures that a single pod exists on each node in the cluster. 
	Every Pod uses the same image. Should a new node be added, the DaemonSet controller will deploy a new Pod on your 
	behalf. Should a node be removed, the controller will delete the Pod also. 
	• The use of a DaemonSet allows for ensuring a particular container is always running. In a large and dynamic environment, it 
	can be helpful to have a logging or metric generation application on every node without an administrator remembering to 
	deploy that application. 
	• Use kind: DaemonSet.
	• As usual, you get all the CRUD operations via kubectl: ​
	$ kubectl get daemonsets
	$ kubectl get ds
	
- StatefulSets : 
	• According to Kubernetes documentation, StatefulSet is the workload API object used to manage stateful applications. Pods 
	deployed using a StatefulSet use the same Pod specification. How this is different than a Deployment is that a StatefulSet
	considers each Pod as unique and provides ordering to Pod deployment. 
	• In order to track each Pod as a unique object, they get an identity composed of stable storage, stable network identity, and 
	an ordinal. This identity remains with the node, regardless to which node the Pod is running on at any one time. 
	• The default deployment scheme is sequential, starting with 0, such as app-0, app-1, app-2, etc. A following Pod will not 
	launch until the current Pod reaches a running and ready state. They are not deployed in parallel.
	• StatefulSets are stable as of Kubernetes v1.9.
	
- Jobs : 
	• Jobs are part of the batch API group. They are used to run a set number of pods to completion. If a pod fails, it will be 
	restarted until the number of completion is reached. 
	• While they can be seen as a way to do batch processing in Kubernetes, they can also be used to run one-off pods. A Job
	specification will have a parallelism and a completion key. If omitted, they will be set to one. If they are present, the 
	parallelism number will set the number of pods that can run concurrently, and the completion number will set how many pods 
	need to run successfully for the Job itself to be considered done. Several Job patterns can be implemented, like a traditional 
	work queue. 
	• Cronjobs work in a similar manner to Linux jobs, with the same time syntax. There are some cases where a job would not be 
	run during a time period or could run twice; as a result, the requested Pod should be idempotent. 
	• An option spec field is .spec.concurrencyPolicy which determines how to handle existing jobs, should the time segment 
	expire. If set to Allow, the default, another concurrent job will be run. If set to Forbid, the current job continues and the new 
	job is skipped. A value of Replace cancels the current job and starts a new job in its place.
	
- Role Based Access Control (RBAC)
	• The last API resources that we will look at are in the rbac.authorization.k8s.io group. We actually have four resources: ClusterRole, Role, 
	ClusterRoleBinding, and RoleBinding. They are used for Role Based Access Control (RBAC) to Kubernetes.
	$ curl localhost:8080/apis/rbac.authorization.k8s.io/v1beta1 
	... 
	"groupVersion": "rbac.authorization.k8s.io/v1beta1", 
	"resources": [ 
	... 
	"kind": "ClusterRoleBinding" 
	... 
	"kind": "ClusterRole" 
	... 
	"kind": "RoleBinding" 
	... 
	"kind": "Role" 
	...
	• These resources allow us to define Roles within a cluster and associate users to these Roles. For example, we can define a Role for someone 
	who can only read pods in a specific namespace, or a Role that can create deployments, but no services.
	
- Autoscaling :
	• In the autoscaling group we find the Horizontal Pod Autoscalers (HPA). This is a stable resource. HPAs automatically scale 
	Replication Controllers, ReplicaSets, or Deployments based on a target of 50% CPU usage by default. The usage is checked 
	by the kubelet every 30 seconds, and retrieved by Heapster every minute. HPA checks with Heapster every 30 seconds. 
	Should a Pod be added or removed, HPA waits 180 seconds before further action. 
	• Other metrics can be used and queried via REST. The autoscaler does not collect the metrics, it only makes a request for the 
	aggregated information and increases or decreases the number of replicas to match the configuration. 
	• The Cluster Autoscaler (CA) adds or removes nodes to the cluster, based on the inability to deploy a Pod or having nodes 
	with low utilization for at least 10 minutes. This allows dynamic requests of resources from the cloud provider and minimizes
	expenses for unused nodes. If you are using CA, nodes should be added and removed through cluster-autoscaler commands. Scale-up and down of nodes is checked every 10 seconds, but decisions are made on a node every 10 minutes. 
	Should a scale-down fail, the group will be rechecked in 3 minutes, with the failing node being eligible in five minutes. The total time to allocate a new node is largely dependent on the cloud provider. 
	• Another project still under development is the Vertical Pod Autoscaler. This component will adjust the amount of CPU and 
	memory requested by Pods.

- Scaling and Rolling Updates :
	• The API server allows for the configurations settings to be updated for most values. There are some immutable values, which may be 
	different depending on the version of Kubernetes you have deployed. 
	• A common update is to change the number of replicas running. If this number is set to zero, there would be no containers, but there 
	would still be a ReplicaSet and Deployment. This is the backend process when a Deployment is deleted. 
	$ kubectl scale deploy/dev-web --replicas=4
	deployment "dev-web" scaled 
	$ kubectl get deployments
	NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE
	dev-web 4 4 4 1 12m 
	• Non-immutable values can be edited via a text editor, as well. Use edit to trigger an update. For example, to change the deployed 
	version of the nginx web server to an older version: 
	$ kubectl edit deployment nginx
	....
	containers:
	- image: nginx:1.8 #<<---Set to an older version
	imagePullPolicy: IfNotPresent
	name: dev-web 
	.... 
	• This would trigger a rolling update of the deployment. While the deployment would show an older age, a review of the Pods would 
	show a recent update and older version of the web server application deployed.
	
- Labels : 
	• Part of the metadata of an object is a label. Though labels are not API objects, they are an important tool for cluster 
	administration. They can be used to select an object based on an arbitrary string, regardless of the object type. Labels are 
	immutable as of API version apps/v1.
	• Every resource can contain labels in its metadata. By default, creating a Deployment with kubectl run adds a label, as we 
	saw in: 
	....
	labels: 
		pod-template-hash: "3378155678"
		run: ghost 
	.... 
	• You could then view labels in new columns: 
	$ kubectl get pods -l run=ghost
	NAME READY STATUS RESTARTS AGE
	ghost-3378155678-eq5i6 1/1 Running 0 10m 
	$ kubectl get pods -Lrun
	NAME READY STATUS RESTARTS AGE RUN
	ghost-3378155678-eq5i6 1/1 Running 0 10m ghost
	nginx-3771699605-4v27e 1/1 Running 1 1h nginx
	• While you typically define labels in pod templates and in the specifications of Deployments, you can also add labels on the 
	fly: 
	$ kubectl label pods ghost-3378155678-eq5i6 foo=bar 
	$ kubectl get pods --show-labels
	NAME READY STATUS RESTARTS AGE LABELS
	ghost-3378155678-eq5i6 1/1 Running 0 11m foo=bar, pod-template-hash=3378155678,run=ghost 
	• For example, if you want to force the scheduling of a pod on a specific node, you can use a nodeSelector in a pod definition, 
	add specific labels to certain nodes in your cluster and use those labels in the pod. 
	....
	spec: 
	containers:
	- image: nginx
	nodeSelector:
	disktype: ssd



























		
		
		
		
		
